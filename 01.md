# agn5
## 用來開啟或關閉某些功能，以便於用戶根據需要進行操作
1. `notebook_mode`
   
    當設定為 `True` 時，表示希望在 Jupyter Notebook環境中運行代碼。這種模式通常會啟用一些特定的功能，如更好的輸出顯示、互動式控制。
   
2. `viz_mode`
   
    當設定為 `True` 時，表示啟用視覺化模式。這涉及生成圖表、圖形或其他視覺化表示，以幫助分析數據或呈現結果。在許多數據科學庫中（如 Matplotlib、Seaborn、Plotly 等），視覺化是分析和解釋數據的重要部分。
```
notebook_mode = True 
viz_mode = True
```
# import
## 常見的數據處理、機器學習和深度學習所需的庫和模組。用於構建和訓練機器學習模型，特別是在處理圖形結構數據。
```
import os #用於與操作系統互動，如文件和目錄的管理
import json #用於解析和生成 JSON（JavaScript Object Notation）格式的數據，為常用的數據交換格式
import argparse #用於解析命令行參數，便於在運行程式時傳入不同的參數設置
import time #提供時間相關的功能，如計時和時間戳

import numpy as np #一個強大的數據處理庫，尤其適合進行數值計算和矩陣運算
import copy #用於複製物件，以便在不影響原始物件的情況下進行操作
import torch #PyTorch 深度學習框架的核心庫，提供張量運算和自動微分功能
from torch.autograd import Variable #用於創建可以跟踪其操作以進行自動微分的張量（新版本 PyTorch已被棄用）
import torch.nn.functional as F #提供神經網絡的各種函數和操作，如激活函數和損失計算
import torch.nn as nn #包含構建神經網絡所需的各種層和模型

import matplotlib #用於數據可視化的庫，通常用於創建靜態、動畫和交互式圖形
import matplotlib.pyplot as plt 

import networkx as nx #用於創建、操作和研究複雜網絡結構的庫，特別是在圖論和網絡科學中非常有用
from sklearn.utils.class_weight import compute_class_weight #用於計算類別權重，以處理不平衡數據集的問題

from tensorboardX import SummaryWriter #用於將 PyTorch 訓練過程中的數據（如損失、準確率）記錄到 TensorBoard 以便可視化
from fastprogress import master_bar, progress_bar #用於快速和簡便地顯示進度條的庫，通常在長時間運行的過程中使用
```
```
# Remove warning
import warnings #用於控制警告訊息的顯示，這裡用來忽略某些類型的警告
warnings.filterwarnings("ignore", category=UserWarning)
from scipy.sparse import SparseEfficiencyWarning #一個特定於 SciPy 的警告，用於處理稀疏矩陣運算的效率
warnings.simplefilter('ignore', SparseEfficiencyWarning)

from config import * # 從`config`模組導入配置參數
from utils.graph_utils import * #從`utils.graph_utils`模組導入圖形處理相關的工具
from utils.google_tsp_reader import GoogleTSPReader #導入 Google TSP 讀取器，可能用於讀取旅行推銷員問題的數據
from utils.plot_utils import * #從`utils.plot_utils`導入可視化工具
from models.gcn_model import ResidualGatedGCNModel #導入自定義的 GCN（圖卷積網絡）模型
from utils.model_utils import *從`utils.model_utils`導入模型相關的工具和函數
```
## 設定Jupyter Notebook環境中的特定行為，用於開發和測試數據科學或機器學習程式碼
* `%load_ext autoreload`:Jupyter Notebook 的magic command，用於加載 autoreload 擴展。這個擴展允許自動重新加載模組，使 Notebook 可以在編輯外部 Python 檔案後立即加載變更，而不需重新啟動內核
* `%autoreload 2`:指示 Notebook 自動重新加載所有導入的模組。這樣在更新模組後，就不必每次手動重新導入或重啟 Notebook
```
if notebook_mode == True:
    %load_ext autoreload 
    %autoreload 2
    %matplotlib inline #指示 Jupyter Notebook 直接在單元格中顯示 matplotlib 圖片，而不是在外部窗口中打開
    from matplotlib_inline.backend_inline import set_matplotlib_formats
    #導入用於設置圖像顯示格式的函數。這在 Jupyter Notebook 中尤其有用，可以更清晰地控制圖像質量
    set_matplotlib_formats('png') #指定 matplotlib 繪製的圖像格式為 png，提高圖像顯示的清晰度
```
# Load configurations (讀取超參數設定)
## 根據運行環境設定不同的配置檔案路徑，並加載相應的配置
* 根據不同模式（`notebook_mode` 和 `viz_mode` 的值）來設置配置文件路徑並加載配置內容，這樣可以適應 Notebook 和命令行環境的需求
* 目的:靈活地根據運行環境來自動選擇適當的配置文件
```
if notebook_mode==False: #代表不是在 Jupyter Notebook 環境下運行
    parser = argparse.ArgumentParser(description='gcn_tsp_parser') #用 argparse 來從命令行接收參數
    parser.add_argument('-c','--config', type=str, default="configs/default.json") #指定配置文件的路徑
    args = parser.parse_args() #解析命令行參數
    config_path = args.config #將配置文件的路徑存儲到 config_path
elif viz_mode == True: #在 Notebook 環境中且 viz_mode 為 True
    config_path = "logs/tsp10/config.json" #為視覺化過程提供特定的配置文件
else: #在 Notebook 中，且 viz_mode 為 False
    config_path = "configs/default.json" #使用默認配置文件
config = get_config(config_path) #加載指定路徑的配置文件，從 JSON 文件中讀取設置，並轉換為 Python 字典或其他數據結構
print("Loaded {}:\n{}".format(config_path, config)) #打印出所加載的配置文件路徑及其內容，方便確認所加載的設置是否符合預期
```
## 針對視覺化模式 (viz_mode == True) 的特定配置
* 設定了一些 GPU 和模型運行的參數，以便在可視化時更便於檢查和調整
* 提供了一些可以開啟的選項來測試不同規模的 TSP 問題，適合在進行可視化或模型驗證時使用
```
if viz_mode==True:
    config.gpu_id = "0" #指定使用 GPU 編號為 0 的設備，指在有多個 GPU 的情況下，系統將在第 0 個 GPU 上運行模型
    config.batch_size = 1 #將批次大小設為 1。在可視化模式中，選擇較小的批次大小以便更快地進行計算，方便查看結果
    config.accumulation_steps = 1 #設梯度累積步驟數為 1。表示模型每次更新權重時，都會直接使用該步的梯度，而不需要累積多個步驟的梯度
    config.beam_size = 1280 #設置光束搜索大小為 1280。這在路徑搜尋或解決組合優化問題時尤其有用，更大的 beam size 可探索更多解，但會增加計算成本
    
    # Uncomment below to evaluate generalization to variable sizes in viz_mode
#     config.num_nodes = 50 #設置節點數目為 50。這是針對TSP圖數據，用於測試模型在不同圖規模下的泛化能力
#     config.num_neighbors = 20 #設置圖中每個節點的鄰居數量為 20。可以用於控制圖的稀疏性，即每個節點僅連接到 20 個其他節點
#     config.train_filepath = f"./data/tsp{config.num_nodes}_train_concorde.txt" #設置訓練數據的文件路徑
#     config.val_filepath = f"./data/tsp{config.num_nodes}_val_concorde.txt" #設置驗證數據的文件路徑
#     config.test_filepath = f"./data/tsp{config.num_nodes}_test_concorde.txt" #設置測試數據的文件路徑
#     以上三個路徑基於節點數目（num_nodes）生成
```
# Configure GPU options
## 設定運行深度學習模型時使用的 GPU
1. `os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"`

    指定 CUDA 裝置的選擇順序。PCI_BUS_ID 是一個設定值，表示系統將根據 GPU 在 PCI 線上的實際硬體順序來分配 CUDA 裝置。這樣可以確保裝置 ID 與實際硬體的排列一致
2. `os.environ["CUDA_VISIBLE_DEVICES"] = str(config.gpu_id)`

   設定了 `CUDA_VISIBLE_DEVICES` 環境變數，用來指定程式可見的 CUDA 裝置。這意味著程式將只會看到並使用 `config.gpu_id` 指定的 GPU 裝置。config.gpu_id 是一個字串變數，代表程式應使用的 GPU ID
```
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID" 
os.environ["CUDA_VISIBLE_DEVICES"] = str(config.gpu_id)
```
## 檢查系統是否有可用的 CUDA 支援 (即 GPU 是否可用)，並根據結果選擇使用 GPU 或 CPU 進行運算
```
if torch.cuda.is_available(): #會返回 `True` 或 `False`，表示是否有可用的 CUDA 設備
    print("CUDA available, using GPU ID {}".format(config.gpu_id)) #告訴用戶 CUDA 是可用的，並且正在使用指定的 GPU ID
    
    dtypeFloat = torch.cuda.FloatTensor
    #將浮點型張量的數據類型設置為 `torch.cuda.FloatTensor`，這樣可以確保張量會在 GPU 上執行計算
    dtypeLong = torch.cuda.LongTensor
    # 將長整數型張量的數據類型設置為 `torch.cuda.LongTensor`，同樣確保這類張量會在 GPU 上運算
    torch.cuda.manual_seed(1) #設置 CUDA 隨機數生成器的種子值，以確保運行過程中的隨機數生成是可重複的，有助於實驗的可再現性
else:
    print("CUDA not available")
    dtypeFloat = torch.FloatTensor #將浮點型張量的數據類型設置為 `torch.FloatTensor`，這樣張量會在 CPU 上進行計算
    dtypeLong = torch.LongTensor #將長整數型張量的數據類型設置為 `torch.LongTensor`，同樣這些張量會在 CPU 上運行
    torch.manual_seed(1) #設置 CPU 上的隨機數生成器的種子值，以確保 CPU 運算中的隨機性也是可重現的
```
# Test data loading (測試讀取一筆資料)
## 主要在 Notebook 模式下運行，負責讀取 TSP 資料集，生成一個批次的資料，並打印出該批次中各種資料的形狀和內容。最後，使用 `plot_tsp` 函數將其中一個樣本的 TSP 圖像繪製出來，以便進行視覺化檢查和分析
```
if notebook_mode: #查是否處於 Notebook 模式
    num_nodes = config.num_nodes #圖中節點的數量，從配置檔讀取
    num_neighbors = config.num_neighbors #每個節點的鄰居數量，從配置檔讀取
    batch_size = config.batch_size #每批次處理的資料量，從配置檔讀取
    #train_filepath = config.train_filepath
    train_filepath = "./data/tsp10_train_concorde_data1.txt"
    #訓練資料的檔案路徑。原本是從配置檔讀取，但被註解掉，改為固定路徑 "./data/tsp10_train_concorde_data1.txt"

    dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, train_filepath)
    #GoogleTSPReader:自定義的資料讀取器，用於讀取 TSP（旅行推銷員問題）的資料
    print("Number of batches of size {}: {}".format(batch_size, dataset.max_iter))
    #dataset.max_iter：表示資料集中可以生成的批次數量。打印出來以確認資料集的大小

    t = time.time() #記錄當前時間，用於計算批次生成所需的時間
    batch = next(iter(dataset))  # Generate a batch of TSPs #從TSP資料集中取得下一個批次的資料
    print("Batch generation took: {:.3f} sec".format(time.time() - t))
    print(batch)
    #打印批次生成所需的時間和批次內容：這有助於了解資料讀取和處理的效率
    print("edges:", batch.edges.shape) #圖的邊資訊，形狀（例如：[batch_size, num_edges, ...]）
    print("edges_values:", batch.edges_values.shape) #邊的權重或距離值
    print("edges_targets:", batch.edges_target.shape) #目標邊，指示在 TSP 解中應該選擇哪些邊
    print("nodes:", batch.nodes.shape) #圖中的節點
    print("nodes_target:", batch.nodes_target.shape) #節點的目標標籤，根據 TSP 最優解設定
    print("nodes_coord:", batch.nodes_coord.shape) #節點的座標，通常表示城市的位置
    print("tour_nodes:", batch.tour_nodes.shape) #旅行路線中的節點順序
    print("tour_len:", batch.tour_len.shape) #行程的總長度
    print("ET",batch.edges_target) #打印 edges_target 的內容，方便檢查目標邊的具體值
    #edges：圖的邊的資訊（即節點之間的連接）
    #edges_values：邊的權重或距離值。
    #edges_targets：目標邊，表示解決 TSP 時最優解中哪條邊應該被選擇。
    #nodes：圖中的節點。
    #nodes_target：節點的目標標籤（根據 TSP 最優解）。
    #nodes_coord：節點的座標（可能代表城市的地理位置）。
    #tour_nodes：旅行推銷員的完整行程路線（節點順序）。
    #tour_len：行程的總長度。
    
    #繪製 TSP 圖像
    idx = 0 #選擇第一個批次中的第一個樣本進行繪圖
    f = plt.figure(figsize=(5, 5)) #創建一個 5x5 英吋的繪圖窗口
    a = f.add_subplot(111) #添加一個子圖，111 表示1行1列中的第1個子圖
    plot_tsp(a, batch.nodes_coord[idx], batch.edges[idx], batch.edges_values[idx], batch.edges_target[idx])
    #a：繪圖的子圖對象
    #batch.nodes_coord[idx]：選定樣本的節點座標
    #batch.edges[idx]：選定樣本的邊資訊
    #batch.edges_values[idx]：選定樣本的邊權重或距離值
    #batch.edges_target[idx]：選定樣本的目標邊
```
# Instantiate model (建立模組)
`ResidualGatedGCNModel(config, dtypeFloat, dtypeLong)`

這是自定義的模型類，基於殘差門控圖卷積網絡（Residual Gated Graph Convolutional Network）。它接收三個參數：

1. config：包含模型的超參數設定。
2. dtypeFloat：浮點數資料型別，根據之前的設定可能是 torch.cuda.FloatTensor 或 torch.FloatTensor。
3. dtypeLong：長整數資料型別，根據之前的設定可能是 torch.cuda.LongTensor 或 torch.LongTensor。
```
if notebook_mode == True: #檢查是否處於 Notebook 模式
    # Instantiate the network
    net = nn.DataParallel(ResidualGatedGCNModel(config, dtypeFloat, dtypeLong))
    #用 PyTorch 的 `DataParallel` 將模型包裝起來，以支持多 GPU 並行運算。這樣可以在多塊 GPU 上同時進行計算，加速訓練過程
    if torch.cuda.is_available(): #檢查系統是否有可用的 CUDA（GPU）設備
        net.cuda() #將模型移動到 GPU 上，以利用 GPU 的計算能力進行加速
    print(net) #輸出模型的結構，包括各層的名稱、參數數量等資訊，幫助開發者了解模型的組成和規模

    # Compute number of network parameters
    nb_param = 0 #初始化參數計數器為 0
    for param in net.parameters(): #遍歷模型的所有參數
        nb_param += np.prod(list(param.data.size())) #計算每個參數張量的元素總數
        #param.data.size():返回張量的形狀，list():將其轉換為列表，np.prod():計算列表中所有元素的乘積，即張量的總元素數量
    print('Number of parameters:', nb_param)
    #將每個參數張量的元素數量累加到 nb_param 中，以獲得模型的總參數數量

    # Define optimizer(定義優化器)
    learning_rate = config.learning_rate #從配置檔中讀取學習率（learning rate）參數，這是優化器用來更新模型權重的步伐大小
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
    #使用 Adam 優化器來優化模型的參數。Adam 是一種基於一階和二階矩估計的自適應學習率優化算法，常用於訓練深度學習模型
    print(optimizer) #輸出優化器的配置，包括學習率和參數等資訊，幫助確認優化器是否正確設置
```
# 測試Train一筆資料
```
#定義機台座標
# def Trans_cor(location):
#     if location == 0 : return [3,6]
#     if location == 1 : return [2,5]
#     if location == 2 : return [4,5]
#     if location == 3 : return [1,4]
#     if location == 4 : return [3,4]
#     if location == 5 : return [5,4]
#     if location == 6 : return [1,3]
#     if location == 7 : return [3,3]
#     if location == 8 : return [5,3]
#     if location == 9 : return [2,2]
#     if location == 10: return [4,2]
#     if location == 11: return [3,1]

#每個 location 對應一組座標，座標值被正規化（例如，3/5 和 6/6），這可能是為了將座標標準化到某個範圍內（如 [0,1]）
def Trans_cor(location):
    if location == 0 : return [3/5,6/6]
    if location == 1 : return [2/5,5/6]
    if location == 2 : return [4/5,5/6]
    if location == 3 : return [1/5,4/6]
    if location == 4 : return [3/5,4/6]
    if location == 5 : return [5/5,4/6]
    if location == 6 : return [1/5,3/6]
    if location == 7 : return [3/5,3/6]
    if location == 8 : return [5/5,3/6]
    if location == 9 : return [2/5,2/6]
    if location == 10: return [4/5,2/6]
    if location == 11: return [3/5,1/6]
" between start and end"
"車輛速率 = 1 (m/min) 所以Manhattan distance為移動時間"
#從座標算出曼哈頓距離
def DS(start,end):
    return sum(map(lambda i ,j : abs(i-j),Trans_cor(start),Trans_cor(end)))
    # Trans_cor(start) 和 Trans_cor(end) 取得 start 和 end 位置的座標
    # map 和 lambda 計算各坐標軸的絕對差值，sum 將所有差值相加得到總距離
```
## 初始化節點和邊權重 (batch.edges_values)
* `batch.edges_values`

   初始化一個三維列表 batch.edges_values，用來存儲每個批次中每個節點對之間的邊權重

   結構: `[batch_size][n][n]`，每個批次有 `n` 個節點，`n` 個節點之間的邊權重初始為 0
```
Node=[0,1,2,3,4,5,6,7,8,9,10,11]
n = 12
batch.edges_values = [[[0 for k in range(n)] for j in range(n)] for i in range(n)]
#batch.edges_values=np.zeros((12,12))

# 自訂屬性 (buffer, idle, car)(個節點定義三個屬性)
buffer=[2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 2, 0] #代表每個節點的緩衝區大小或容量
idle=[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0] #表示節點是否閒置（1 表示閒置，0 表示不閒置）
car=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] #表示節點是否有車輛（1 表示有，0 表示無）

# 計算邊的權重（曼哈頓距離）
#  功能：填充 batch.edges_values，計算每對節點之間的曼哈頓距離
for i in range(len(Node)):
    for j in range(len(Node)):
        if i==j:
            batch.edges_values[i][j]=0  #如果 i == j，即同一節點，邊權重設為 0
        else:
            first=Node[i]
            second=Node[j]
            batch.edges_values[i][j]=DS(first,second)
            #否則，計算 Node[i] 和 Node[j] 之間的曼哈頓距離，並賦值給 batch.edges_values[i][j]
batch.edges_values=[batch.edges_values] #最後，將 batch.edges_values 包裝成一個列表，形成批次結構

#print(batch.edges_values)

#準備節點座標 (batch.nodes_coord)
#  功能：為每個節點組合 buffer、idle 和 car 屬性，形成節點的座標資訊
batch.nodes_coord=[]

for i in range(len(Node)):
    temp=[]
    temp.insert(0,buffer[i])
    temp.insert(1,idle[i])
    temp.insert(2,car[i])  #創建一個臨時列表 temp，依次插入 buffer[i]、idle[i] 和 car[i]
    batch.nodes_coord.append(temp) #將 temp 添加到 batch.nodes_coord 中
batch.nodes_coord=[batch.nodes_coord]  #最後，將 batch.nodes_coord 包裝成一個列表，形成批次結構
print(batch.nodes_coord) #打印 batch.nodes_coord 以確認內容


#定義邊和目標邊
#  功能：手動設置邊的連接關係和目標邊

#定義圖中節點之間的連接關係，使用二維列表表示。每個子列表代表一個節點的連接情況（1 表示有連接，0 表示無連接）
batch.edges=[[0,1,1,0,0,0,0,0,0,0,0,0],[0,0,0,1,1,1,0,0,0,0,0,0],[0,0,0,1,1,1,0,0,0,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0]]
batch.edges=[batch.edges]
batch.nodes=[[1,1,1,1,1,1,1,1,1,1,1,1]]

#義目標邊，用於訓練模型識別哪些邊應該被選擇。用於有監督學習中作為標籤
batch.edges_target=[[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0]]

#原本是全為 0 的目標邊，後來修改為有特定的目標邊
#batch.edges_target=[[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0]]
batch.edges_target=[batch.edges_target]


# 輸入變數轉換為 PyTorch 變量
#  功能：將資料轉換為 PyTorch 的 Variable，以便於模型處理
#Input variables
x_edges = Variable(torch.LongTensor(batch.edges).type(dtypeLong), requires_grad=False)
#邊的連接信息，轉換為長整數型張量
x_edges_values = Variable(torch.FloatTensor(batch.edges_values).type(dtypeFloat), requires_grad=False)
#邊的權重（曼哈頓距離），轉換為浮點數型張量
x_nodes = Variable(torch.LongTensor(batch.nodes).type(dtypeLong), requires_grad=False)
#節點的標識，轉換為長整數型張量
x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)
#節點的座標資訊（buffer、idle、car），轉換為浮點數型張量
y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)
#目標邊，轉換為長整數型張量
#requires_grad=False：這些變量在訓練過程中不需要計算梯度，因為它們是輸入數據
##

#print(batch)

#計算類別權重
#  功能：計算類別權重，用於處理類別不平衡問題
# Compute class weights
edge_labels = y_edges.cpu().numpy().flatten() #將 y_edges 從 GPU 移回 CPU，轉換為 NumPy 陣列並展平成一維
edge_cw = compute_class_weight("balanced", classes=np.unique(edge_labels), y=edge_labels)
#compute_class_weight：來自 sklearn 的函數，用於計算每個類別的權重。"balanced" 模式會根據類別出現的頻率自動調整權重，較少出現的類別權重較大
#dge_cw：計算得到的類別權重，用於後續的損失函數計算

#模型前向傳播
y_preds, q = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)
#y_preds：模型的預測輸出
#q：可能是模型的中間結果或附加輸出（具體取決於模型實現）
#調用模型的前向傳播方法，輸入包括邊的連接信息、邊的權重、節點標識、節點座標、目標邊和類別權重
#loss = loss.mean()
print("Output size: {}".format(y_preds.size())) #打印預測結果的尺寸，幫助檢查模型輸出的正確性
#x_edges:指示函數，x_edges_value:distance matrix，
```
## 定義連接函數 (connect 函數)
```
#功能：根據給定的位置編號，返回對應的節點連接關係
#每個 location 對應一對節點 [a, b]，表示這對節點之間存在連接
#幾號工單 回傳所對應的節點
def connect(location):
    if location == 0 : return [0,1]
    if location == 1 : return [0,2]
    if location == 2 : return [1,3]
    if location == 3 : return [1,4]
    if location == 4 : return [1,5]
    if location == 5 : return [2,3]
    if location == 6 : return [2,4]
    if location == 7 : return [2,5]
    if location == 8 : return [3,6]
    if location == 9 : return [3,7]
    if location == 10: return [3,8]
    if location == 11 : return [4,6]
    if location == 12 : return [4,7]
    if location == 13 : return [4,8]
    if location == 14 : return [5,6]
    if location == 15 : return [5,7]
    if location == 16 : return [5,8]
    if location == 17 : return [6,9]
    if location == 18 : return [6,10]
    if location == 19 : return [7,9]
    if location == 20 : return [7,10]
    if location == 21 : return [8,9]
    if location == 22: return [8,10]
    if location == 23 : return [9,11]
    if location == 24 : return [10,11]
```
# Reward Function
```
#選擇Goal則給獎勵10，其餘皆-1
#功能：這個函數根據當前狀態 (state)、可行動集合 (actionset) 和選擇的動作 (action) 來返回獎勵
#  獎勵矩陣 reward 是一個 12x12 的二維列表，預設所有動作的獎勵為 -1
#  當選擇到達目標 (Goal) 的動作時，給予獎勵 10
#  具體來說，第 10 和第 11 行的最後一個元素被設為 10，表示這些動作是達成目標的動作
def Find_reward(state,actionset,action):

    reward=[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10],[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10],[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]

    
    return reward
```
# TEST ONE DATA (TEST Function)
```
#初始化距離矩陣 (edges_values)
#  功能：初始化並計算節點之間的曼哈頓距離，存入 batch.edges_values
global edges_value
Node=[0,1,2,3,4,5,6,7,8,9,10,11]
n=12
#創建一個 12x12x12 的三維列表 batch.edges_values，初始值全部為 0
batch.edges_values = [[[0 for k in range(n)] for j in range(n)] for i in range(n)] #Distance matrix固定不變

#用雙層迴圈遍歷所有節點對 (i, j)，若 i == j，距離設為 0
#否則，計算節點 i 和節點 j 之間的曼哈頓距離 DS(first, second)，並存入 batch.edges_values[i][j]
for i in range(len(Node)):
    for j in range(len(Node)):
        if i==j:
            batch.edges_values[i][j]=0
        else:
            first=Node[i]
            second=Node[j]
            batch.edges_values[i][j]=DS(first,second)
            
edges_value = batch.edges_values
```
```
#測試單筆資料函數 (test_one_data_w)
#  功能：測試單筆資料，根據輸入的座標、鄰接矩陣和可行動集合，輸出選擇的動作和對應的 Q 值(Q-Learning)
def test_one_data_w(coord,adjacency_w,action_s):
#  coord：節點的座標資訊
#  adjacency_w：當前狀態下的鄰接矩陣
#  action_s：可行動集合，即當前可以選擇的動作
    global edges_value
    # 資料前處理 (Encoding)
    batch.edges=[[0,1,1,0,0,0,0,0,0,0,0,0],[0,0,0,1,1,1,0,0,0,0,0,0],[0,0,0,1,1,1,0,0,0,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0]]
    #batch.edges 為預設的鄰接矩陣

    #batch.edges=[[2,1,1,0,0,0,0,0,0,0,0,0],[0,2,0,1,1,1,0,0,0,0,0,0],[0,0,2,1,1,1,0,0,0,0,0,0],[0,0,0,2,0,0,1,1,1,0,0,0],[0,0,0,0,2,0,1,1,1,0,0,0],[0,0,0,0,0,2,1,1,1,0,0,0],[0,0,0,0,0,0,2,0,0,1,1,0],[0,0,0,0,0,0,0,2,0,1,1,0],[0,0,0,0,0,0,0,0,2,1,1,0],[0,0,0,0,0,0,0,0,0,2,0,1],[0,0,0,0,0,0,0,0,0,0,2,1],[0,0,0,0,0,0,0,0,0,0,0,2]]
    batch.edges=[batch.edges] #指示函數不變，將 batch.edges 包裝成列表
    #batch.edges=adjacency_w  #目前的state_input_edges
    #設定 batch.edges_values、batch.nodes、batch.nodes_coord 和 batch.edges_target
    batch.edges_values=[edges_value] #Distance matrix
    batch.nodes=[[1,1,1,1,1,1,1,1,1,1,1,1]]
    batch.nodes_coord=[coord] #目前的state_input_nodes
    batch.edges_target=[adjacency_w]#目前的state_input_edges

#將 batch.edges、batch.edges_values、batch.nodes、batch.nodes_coord 和 batch.edges_target 轉換為 PyTorch 的 Variable，並指定資料型別
    x_edges = Variable(torch.LongTensor(batch.edges).type(dtypeLong), requires_grad=False)
    x_edges_values = Variable(torch.FloatTensor(batch.edges_values).type(dtypeFloat), requires_grad=False)
    x_nodes = Variable(torch.LongTensor(batch.nodes).type(dtypeLong), requires_grad=False)
    x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)
    y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)
    y_nodes = Variable(torch.LongTensor(batch.nodes_target).type(dtypeLong), requires_grad=False)

#前向傳播：將處理好的資料輸入模型 net，獲得預測結果 y_preds 和 Q 值 q
    # Forward pass (輸入網路架構，輸出Q值)
    edge_cw=1
    y_preds, q = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)

#選擇最大 Q 值的動作
    q=q.reshape(12,12) #將 Q 值 q 重塑為 12x12 的矩陣
    max_num=torch.argmax(q) #找出 Q 值中最大的元素位置 max_num
    max_num=max_num.item() #濾出 action_set 中的 Q 值，並選擇其中最大的 Q 值對應的動作 Max_choose
    
    #Filter (只採用action_set的Q值)
    in_action_set=[]
    for i in range(len(action_s)):
        position=connect(action_s[i])
        in_action_set.append(q[position[0]][position[1]])

    Max_q=max(in_action_set)
    Max_choose=action_s[in_action_set.index(max(in_action_set))]    

    return [Max_choose,Max_q] #返回選擇的動作 Max_choose 和對應的 Q 值 Max_q
```
# Train One Data (Train Function)
```
#函數定義和參數
#  功能：訓練單筆資料，根據當前狀態和動作集合，更新模型的 Q 值
#  參數：
#  state_now：當前狀態
#  action_set_now：當前狀態下的可行動集合
#  choose_action：選擇的動作
#  state_next：執行動作後的下一個狀態
#  action_set_next：下一個狀態下的可行動集合
def Train_One_Data(state_now,action_set_now,choose_action,state_next,action_set_next):
    global edges_value, loss
    edge_cw = None
#檢查下一個動作集合是否為空
#  功能：如果下一個狀態下沒有可行動集合，則不進行訓練，直接返回
#  原因：避免在沒有可行動的情況下進行更新，防止錯誤
    if len(action_set_next)==0:#若下一個action_set為空，則不學此筆資料
        return;
    #讀取現在的state

    #重新定義機台0和機台11的值
    #  功能：限制機台0的值不超過2，並將機台11的值設為0
    if state_now[0][0] > 2: 
        state_now[0][0]=2
    state_now[11][0]=0
    
    #將action_set_now轉成鄰接矩陣 (adjacency_now)
    #  功能：根據當前的可行動集合 action_set_now，生成當前狀態下的鄰接矩陣 adjacency_now
    adjacency_now=[]    
    temp=np.zeros((12,12)) #創建一個 12x12 的零矩陣 temp
    for i in range(len(action_set_now)):
        num=int(action_set_now[i])
        nodes=connect(num)
        #遍歷 action_set_now 中的每個動作編號 num，使用 connect(num) 函數獲取對應的節點對 [a, b]
        temp[nodes[0]][nodes[1]]=1
        #將 temp[a][b] 設為1，表示節點 a 和節點 b 之間有連接
    adjacency_now.append(temp) #將 temp 添加到 adjacency_now 列表中
    
    #處理下一個狀態 (state_next) 和下一個動作集合 (action_set_next)
    #重新定義機台0和機台11的值
    if state_next[0][0] > 2: 
        state_next[0][0]=2
    state_next[11][0]=0
    #限制 state_next[0][0] 不超過2，並將 state_next[11][0] 設為0

    #將action_set_next轉成鄰接矩陣 adjacency_next
    adjacency_next=[]    
    temp=np.zeros((12,12))
    for i in range(len(action_set_next)):
        num=int(action_set_next[i])
        nodes=connect(num)
        temp[nodes[0]][nodes[1]]=1
    adjacency_next.append(temp) #根據 action_set_next 中的動作編號，生成下一個狀態下的鄰接矩陣 adjacency_next
##################Input Now #######################################################################################
#功能：設置當前狀態下的鄰接矩陣、距離矩陣、節點標識、節點座標和目標鄰接矩陣
    batch.edges=[[0,1,1,0,0,0,0,0,0,0,0,0],[0,0,0,1,1,1,0,0,0,0,0,0],[0,0,0,1,1,1,0,0,0,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,1,1,0],[0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0]] #設置為預設的鄰接矩陣，並包裝成列表
    batch.edges=[batch.edges] #指示函數不變
    #batch.edges=adjacency_now
    batch.edges_values=[edges_value] #Distance matrix，batch.edges_values：設置為全局變數 ；edges_value，即距離矩陣
    batch.nodes=[[1,1,1,1,1,1,1,1,1,1,1,1]] #設置節點標識為全1列表
    batch.nodes_coord=[state_now] #目前的state_input_nodes #設置節點座標為當前狀態 state_now
    batch.edges_target=[adjacency_now]#目前的state_input_edges #設置目標鄰接矩陣為當前的鄰接矩陣 adjacency_now
####################AGN############AGN###############AGN##################AGN#############AGN##################AGN#############
# 轉換為 PyTorch 變量並計算類別權重，並指定資料型別和是否需要計算梯度
#  將批次資料轉換為 PyTorch 的 Variable，以便於模型處理
    # Convert batch to torch Variables
    x_edges = Variable(torch.LongTensor(batch.edges).type(dtypeLong), requires_grad=False)
    x_edges_values = Variable(torch.FloatTensor(batch.edges_values).type(dtypeFloat), requires_grad=False)
    x_nodes = Variable(torch.LongTensor(batch.nodes).type(dtypeLong), requires_grad=False)
    x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)
    y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)
    y_nodes = Variable(torch.LongTensor(batch.nodes_target).type(dtypeLong), requires_grad=False)

    #使用 compute_class_weight 計算類別權重 edge_cw，使得不同類別的權重平衡
    if type(edge_cw) != torch.Tensor: #如果類別權重 edge_cw 尚未計算，則計算類別權重以處理類別不平衡問題
        edge_labels = y_edges.cpu().numpy().flatten()
        edge_cw = compute_class_weight("balanced", classes=np.unique(edge_labels), y=edge_labels)
        
    # Forward pass前向傳播
    #  功能：將處理好的資料輸入模型 net，進行前向傳播，獲得預測結果 y_preds 和 Q 值 q
    y_preds, q = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)
############################################################################################################################    
#Next State Data處理下一個狀態的資料
#  功能：處理下一個狀態 state_next，並進行前向傳播，獲得下一個 Q 值 q2
    #batch.edges=adjacency_next:更新 batch.nodes_coord 和 batch.edges_target 為下一個狀態的資料
    batch.nodes_coord=[state_next] #下一筆的state_input_nodes
    batch.edges_target=[adjacency_next]#下一筆的state_input_edges

    #轉換為 PyTorch 的變量 x_nodes_coord 和 y_edges
    x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)
    y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)
    #進行前向傳播，獲得 y_preds2 和 q2
    y_preds2, q2 = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw) #下一筆的 Q
    #複製 q2 為 q2_original，並重塑為 12x12 的矩陣
    q2_original=q2.clone()
    q2=q2.reshape(12,12)
    q2_original=q2_original.reshape(12,12)

    #選擇下一個動作並計算獎勵
    #  功能：從下一個狀態的 Q 值中選擇可行動集合中最大的 Q 值對應的動作，並計算該動作的獎勵 reward2
    #Filter 找出q2的 a'(q2max_choose)
    in_action_set=[]
    for i in range(len(action_set_next)):
    #遍歷 action_set_next，獲取每個動作在 Q 值矩陣中的位置，並將對應的 Q 值加入 in_action_set
        position=connect(action_set_next[i])
        in_action_set.append(q2[position[0]][position[1]])

    q2max_choose=action_set_next[in_action_set.index(max(in_action_set))]
    #選擇 in_action_set 中最大的 Q 值 Max_q 對應的動作 Max_choose
    
    reward2=Find_reward(state_next,action_set_next,q2max_choose) #根據目前State找出Reward
    #使用 Find_reward 函數計算該動作的獎勵 reward2
    #print("Q2",q2)


    #找出Q2的max值
    #  功能：找出 q2 中最大的 Q 值及其位置            
    max_num=torch.argmax(q2) #使用 torch.argmax(q2) 找出 q2 中最大的元素的位置
    max_num=max_num.item() #將位置轉換為索引，並獲取對應的 Q 值 max_q2_values
    max_q2_values=q2[max_num//12][max_num%12].item()
    
####Update Q#############Update Q############Update Q#############Update Q############Update Q##################Update Q###
#更新 Q 值
#  根據 Q-Learning 的更新規則，更新目標 Q 值 target_q
    original_q=q.clone() #複製並重塑當前的 Q 值 q 為 original_q
    original_q=original_q.reshape(12,12) #12*12

    #計算當前動作 choose_action 的獎勵 reward1
    reward1=Find_reward(state_now,action_set_next,choose_action) #根據目前State找出Reward

    #複製並重塑 q 為 target_q，並分離計算圖 (detach)
    target_q=q.clone().detach()

    target_q=target_q.reshape(12,12) #12*12
    #print("target_q:",target_q)
    #print("action_choose:",action_choose[batch_num+epoch* batches_per_epoch])
    #print("action_choose:",action_choose[1377])

    #使用 connect(choose_action) 獲取選擇動作的節點對 [i, j]
    action_ij=connect(choose_action) #找出action_ij

    #print("action_ij",action_ij)
    #計算目標 Q 值 target_q_values，根據獎勵和下一個狀態的最大 Q 值
    #  Q target(i,j)=reward1(i,j)+γ×max_q2_values，其中 γ=0.9 是折扣因子
    target_q_values=reward1[action_ij[0]][action_ij[1]]+ 0.9 * max_q2_values
    target_q[action_ij[0]][action_ij[1]] = target_q_values #更新 target_q[i][j] 為 target_q_values
###################################################################################################################################
#計算損失並反向傳播
#Compute Loss and backward net
#  計算損失函數，進行反向傳播並更新模型參數
    #print("QQ",original_q)
    #使用 F.smooth_l1_loss 計算 original_q 與 target_q 之間的損失，這是一種結合了 L1 和 L2 損失的平滑損失函數
    loss = F.smooth_l1_loss(original_q, target_q)
    loss = loss.mean() #計算損失的平均值 loss.mean()
    Loss_plt.append(loss.item()) #將損失值添加到 Loss_plt 列表中，用於後續的損失可視化
    optimizer.zero_grad() #重置優化器的梯度 optimizer.zero_grad()
    #print("loss:",loss)
    loss.backward() #執行反向傳播 loss.backward()，計算梯度
    optimizer.step() #更新模型參數 optimizer.step()
```
